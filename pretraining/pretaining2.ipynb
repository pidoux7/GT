{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:52:43.107570Z",
     "start_time": "2024-01-11T15:52:40.996704Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as tgmnn\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.loader import DataListLoader as GraphLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import sklearn.metrics as skm\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pytorch_pretrained_bert as Bert\n",
    "import itertools\n",
    "from einops import rearrange, repeat\n",
    "import ast\n",
    "from typing import Optional, Tuple, Union\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, SparseTensor\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn import LayerNorm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import transformers\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:07:59.772788Z",
     "start_time": "2024-01-11T16:07:59.740421Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerConv(MessagePassing):\n",
    "    _alpha: OptTensor\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        heads: int = 1,\n",
    "        concat: bool = True,\n",
    "        beta: bool = False,\n",
    "        dropout: float = 0.,\n",
    "        edge_dim: Optional[int] = None,\n",
    "        bias: bool = True,\n",
    "        root_weight: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.beta = beta and root_weight\n",
    "        self.root_weight = root_weight\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.edge_dim = edge_dim\n",
    "        self._alpha = None\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_key = Linear(in_channels[0], heads * out_channels)\n",
    "        self.lin_query = Linear(in_channels[1], heads * out_channels)\n",
    "        self.lin_value = Linear(in_channels[0], heads * out_channels)\n",
    "        self.layernorm1 = LayerNorm(out_channels)\n",
    "        self.layernorm2 = LayerNorm(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = Linear(heads * out_channels, out_channels)\n",
    "        self.ffn = Linear(out_channels, out_channels)\n",
    "        self.ffn2 = Linear(out_channels, out_channels)\n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
    "        else:\n",
    "            self.lin_edge = self.register_parameter('lin_edge', None)\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.lin_key.reset_parameters()\n",
    "        self.lin_query.reset_parameters()\n",
    "        self.lin_value.reset_parameters()\n",
    "        if self.edge_dim:\n",
    "            self.lin_edge.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, batch=None, return_attention_weights=None):\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        r\"\"\"Runs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "        residual = x\n",
    "        x = self.layernorm1(x, batch)\n",
    "        if isinstance(x, Tensor):\n",
    "            x: PairTensor = (x, x)\n",
    "        query = self.lin_query(x[1]).view(-1, H, C)\n",
    "        key = self.lin_key(x[0]).view(-1, H, C)\n",
    "        value = self.lin_value(x[0]).view(-1, H, C)\n",
    "        # propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa\n",
    "        out = self.propagate(edge_index, query=query, key=key, value=value,\n",
    "                             edge_attr=edge_attr, size=None)\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "        if self.concat:\n",
    "            out = self.proj(out.view(-1, self.heads * self.out_channels))\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out = out+residual\n",
    "        residual = out\n",
    "\n",
    "        out = self.layernorm2(out)\n",
    "        out = self.gelu(self.ffn(out))\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out = self.ffn2(out)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out = out + residual\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n",
    "                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "\n",
    "\n",
    "        if self.lin_edge is not None:\n",
    "            assert edge_attr is not None\n",
    "            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads,\n",
    "                                                      self.out_channels)\n",
    "            key_j = key_j + edge_attr\n",
    "\n",
    "        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = value_j\n",
    "        if edge_attr is not None:\n",
    "            out = out + edge_attr\n",
    "\n",
    "        out = out * alpha.view(-1, self.heads, 1)\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformerconv1 = TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=True)\n",
    "        self.transformerconv2 = TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=True)\n",
    "        self.transformerconv3 = TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=False)\n",
    "\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size // 5)\n",
    "        self.embed_ee = nn.Embedding(config.node_attr_size, config.hidden_size // 5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_index_readout, edge_attr, batch):\n",
    "        indices = (x==0).nonzero().squeeze()\n",
    "        h_nodes = self.transformerconv1(x=self.embed(x), edge_index=edge_index, edge_attr=self.embed_ee(edge_attr), batch=batch)\n",
    "        h_nodes = nn.GELU()(h_nodes)\n",
    "        h_nodes = self.transformerconv2(x=h_nodes, edge_index=edge_index, edge_attr=self.embed_ee(edge_attr), batch=batch)\n",
    "        h_nodes = nn.GELU()(h_nodes)\n",
    "        h_nodes = self.transformerconv3(x=h_nodes, edge_index=edge_index, edge_attr=self.embed_ee(edge_attr), batch=batch)\n",
    "        x = h_nodes[indices]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, segment, age\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        #self.word_embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "        self.word_embeddings = GraphTransformer(config)\n",
    "        self.type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size//5)\n",
    "        self.age_embeddings = nn.Embedding(config.age_vocab_size, config.hidden_size//5). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.age_vocab_size, config.hidden_size//5))\n",
    "        self.time_embeddings = nn.Embedding(config.time_vocab_size , config.hidden_size//5). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.time_vocab_size, config.hidden_size//5))\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size//5). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size//5))\n",
    "\n",
    "\n",
    "        self.seq_layers = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.acti = nn.GELU()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "\n",
    "    def forward(self, nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids,  type_ids, posi_ids):\n",
    "        word_embed = self.word_embeddings(nodes, edge_index, edge_index_readout, edge_attr, batch)\n",
    "\n",
    "        type_embeddings = self.type_embeddings(type_ids)\n",
    "        age_embed = self.age_embeddings(age_ids)\n",
    "        time_embeddings = self.time_embeddings(time_ids)\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "\n",
    "\n",
    "        word_embed = torch.reshape(word_embed, type_embeddings.shape)\n",
    "        embeddings = torch.cat((word_embed, type_embeddings, posi_embeddings, age_embed, time_embeddings), dim=2)\n",
    "        b, n, _ = embeddings.shape\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "        embeddings = self.seq_layers(embeddings)\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "\n",
    "##%%\n",
    "\n",
    "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config=config)\n",
    "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
    "        self.pooler = Bert.modeling.BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, attention_mask=None,\n",
    "                output_all_encoded_layers=True):\n",
    "        #print(\"bert model\")\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(age_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "##%%\n",
    "\n",
    "class BertForMTR(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMTR, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        #self.gru = nn.GRU(config.hidden_size, config.hidden_size // 2, 1, batch_first = True, bidirectional=True)\n",
    "        #self.gru = nn.Linear(config.hidden_size * 50, config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.apply(self.init_bert_weights)\n",
    "    def forward(self, nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, attention_mask=None, labels=None, masks=None):\n",
    "        _, pooled_output = self.bert(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        #pooled_output = self.dropout(pooled_output)\n",
    "        #pooled_output = pooled_output * attention_mask.unsqueeze(-1)\n",
    "        #pooled_output = torch.sum(pooled_output, axis=1) / torch.sum(attention_mask, axis=1).unsqueeze(-1)\n",
    "        #pooled_output = torch.mean(_, axis=1)\n",
    "        #pooled_output, x = self.gru(pooled_output)\n",
    "        #pooled_output = self.gru(torch.flatten(pooled_output, start_dim=1))\n",
    "        #pooled_output = self.relu(self.dropout(pooled_output))\n",
    "        logits = self.classifier(pooled_output).squeeze(dim=1)\n",
    "        bce_logits_loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        discr_supervised_loss = bce_logits_loss(logits, labels)\n",
    "\n",
    "        return discr_supervised_loss, logits\n",
    "\n",
    "##%%\n",
    "\n",
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.type_vocab_size = config.get('type_vocab_size')\n",
    "        self.time_vocab_size = config.get('time_vocab_size')\n",
    "        self.graph_dropout_prob = config.get('graph_dropout_prob')\n",
    "        self.node_attr_size = config.get('node_attr_size')\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')\n",
    "\n",
    "##%%\n",
    "\n",
    "class GDSet(Dataset):\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        g = self.g[index]\n",
    "        for i in range(len(g)):\n",
    "          g[i]['posi_ids'] = i\n",
    "        return g\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:59:06.641769Z",
     "start_time": "2024-01-11T15:52:43.128940Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(path + 'data_pad.pkl', 'rb') as handle:\n",
    "    dataset_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:59:06.645176Z",
     "start_time": "2024-01-11T15:59:06.642356Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataset = dataset_loaded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:59:07.406963Z",
     "start_time": "2024-01-11T15:59:06.644556Z"
    }
   },
   "outputs": [],
   "source": [
    "#with open('data_pad100', \"wb\") as f:\n",
    "    #pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:59:07.409236Z",
     "start_time": "2024-01-11T15:59:07.407511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6132\n",
      "50\n",
      "Data(subject_id=[1], hadm_id=[1], label=[1], age=[1], rang=[1], type=[1], x=[56], edge_index=[2, 1540], edge_attr=[1540], mask_v=[1], time=[1])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(dataset[0]))\n",
    "print(dataset[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:59:07.470934Z",
     "start_time": "2024-01-11T15:59:07.459634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 3764\n",
      "max noeud 9403\n",
      "node_attr_size 8\n",
      "max edge_attr 7\n",
      "age_size 74\n",
      "max age 130\n",
      "time_size 367\n",
      "max time 367\n",
      "type_size 11\n",
      "max type 10\n",
      "label_size 2\n",
      "max label 1\n",
      "hadm_size 24138\n",
      "subject_size 6132\n",
      "maskv_size 2\n",
      "max maskv 1\n",
      "rang_size 51\n",
      "max rang 51\n"
     ]
    }
   ],
   "source": [
    "noeud_unique = set()\n",
    "edge_attr_unique = set()\n",
    "age_unique = set()\n",
    "time_unique = set()\n",
    "type_unique = set()\n",
    "label_unique = set()\n",
    "hadm_unique = set()\n",
    "subject_unique = set()\n",
    "mask_v_unique = set()\n",
    "rang_unique = set()\n",
    "for patient in dataset:\n",
    "    for visite in patient:\n",
    "        noeuds = visite.x.tolist()\n",
    "        edge = visite.edge_attr.tolist()\n",
    "        label = visite.label.tolist()\n",
    "        age = visite.age.tolist()\n",
    "        time = visite.time.tolist()\n",
    "        typ = visite.type.tolist()\n",
    "        mask_v = visite.mask_v.tolist()\n",
    "        rang = visite.rang.tolist()\n",
    "        hadm = visite.hadm_id.tolist()\n",
    "        subject = visite.subject_id.tolist()\n",
    "        for noeud in noeuds:\n",
    "            noeud_unique.add(noeud)\n",
    "        for attribut in edge:\n",
    "            edge_attr_unique.add(attribut)\n",
    "        for lab in label:\n",
    "            label_unique.add(lab)\n",
    "        for a in age:\n",
    "            age_unique.add(a)\n",
    "        for t in time:\n",
    "            time_unique.add(t)\n",
    "        for ty in typ:\n",
    "            type_unique.add(ty)\n",
    "        for mask in mask_v:\n",
    "            mask_v_unique.add(mask)\n",
    "        for r in rang:\n",
    "            rang_unique.add(r)\n",
    "        for h in hadm:\n",
    "            hadm_unique.add(h)\n",
    "        for s in subject:\n",
    "            subject_unique.add(s)\n",
    "        \n",
    "\n",
    "vocab_size = len(noeud_unique)\n",
    "edge_attr_size = len(edge_attr_unique)\n",
    "age_size = len(age_unique)\n",
    "time_size = len(time_unique)\n",
    "type_size = len(type_unique)\n",
    "label_size = len(label_unique)\n",
    "hadm_size = len(hadm_unique)\n",
    "subject_size = len(subject_unique)\n",
    "mask_v_size = len(mask_v_unique)\n",
    "rang_size = len(rang_unique)\n",
    "\n",
    "print('vocab_size',vocab_size)\n",
    "print('max noeud',max(noeud_unique))\n",
    "print('node_attr_size',edge_attr_size)\n",
    "print('max edge_attr',max(edge_attr_unique))\n",
    "print('age_size',age_size)\n",
    "print('max age',max(age_unique))\n",
    "print('time_size',time_size)\n",
    "print('max time',max(time_unique))\n",
    "print('type_size',type_size)\n",
    "print('max type',max(type_unique))\n",
    "print('label_size',label_size)\n",
    "print('max label',max(label_unique))\n",
    "print('hadm_size',hadm_size)\n",
    "print('subject_size',subject_size)\n",
    "print('maskv_size',mask_v_size)\n",
    "print('max maskv',max(mask_v_unique))\n",
    "print('rang_size',rang_size)\n",
    "print('max rang',max(rang_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:59:07.473371Z",
     "start_time": "2024-01-11T15:59:07.471502Z"
    }
   },
   "outputs": [],
   "source": [
    "train_l = int(len(dataset)*0.80)\n",
    "val_l = int(len(dataset)*0.10)\n",
    "test_l = len(dataset) - val_l - train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T15:59:07.477934Z",
     "start_time": "2024-01-11T15:59:07.474467Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, random_seed=1, few_shots=1):\n",
    "  rs = ShuffleSplit(n_splits=1, test_size=.10, random_state=random_seed)\n",
    "\n",
    "  k = 5\n",
    "\n",
    "  for i, (train_index_tmp, test_index) in enumerate(rs.split(dataset)):\n",
    "    rs2 = ShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=random_seed)\n",
    "    for j, (train_index, val_index) in enumerate(rs2.split(train_index_tmp)):\n",
    "      train_index = train_index_tmp[train_index]\n",
    "      if few_shots < 1:\n",
    "        train_index = random.sample(list(train_index), int(len(train_index) * few_shots))\n",
    "      val_index = train_index_tmp[val_index]\n",
    "\n",
    "      trainDSet = [dataset[x] for x in train_index]\n",
    "      valDSet = [dataset[x] for x in val_index]\n",
    "      testDSet = [dataset[x] for x in test_index]\n",
    "      return trainDSet, valDSet, testDSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:05:06.215977Z",
     "start_time": "2024-01-11T16:05:06.210962Z"
    }
   },
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 50,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 5,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': \"cpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    'data_len' : len(dataset),\n",
    "    'train_data_len' : train_l,\n",
    "    'val_data_len' : val_l,\n",
    "    'test_data_len' : test_l,\n",
    "    'epochs' : 3\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': 9405, # number of disease + symbols for word embedding\n",
    "    'edge_relationship_size': 8, # number of vocab for edge_attr\n",
    "    'hidden_size': 50*5, # word embedding and seg embedding hidden size\n",
    "    'age_vocab_size': 151, # number of vocab for age embedding\n",
    "    'time_vocab_size': 380, # number of vocab for time embedding\n",
    "    'type_vocab_size': 11, # number of vocab for type embedding\n",
    "    'node_attr_size': 8, # number of vocab for node_attr embedding\n",
    "    'num_labels': 1,\n",
    "    'max_position_embedding': 50, # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'graph_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 2, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.2, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "    'n_layers' : 3 - 1,\n",
    "    'alpha' : 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fonction entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:12:55.505582Z",
     "start_time": "2024-01-11T16:12:55.470667Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(e, model, optim_model, trainload, device):\n",
    "    tr_loss = 0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for step, data in tqdm(enumerate(trainload)):\n",
    "        optim_model.zero_grad()\n",
    "\n",
    "        batched_data = Batch()\n",
    "        graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
    "        graph_batch = graph_batch.to(device)\n",
    "        nodes = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        edge_index_readout = graph_batch.edge_index\n",
    "        edge_attr = graph_batch.edge_attr\n",
    "        batch = graph_batch.batch\n",
    "        age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
    "        time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
    "        type_ids = torch.reshape(graph_batch.type, [graph_batch.type.shape[0] // 50, 50])\n",
    "        posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
    "        attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
    "        attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
    "\n",
    "        labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, 0].float()\n",
    "        #masks = torch.reshape(graph_batch.mask, [graph_batch.mask.shape[0] // 50, 50])[:, 0]\n",
    "        loss, logits = model(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids,type_ids,posi_ids,attMask, labels)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        #if step%500 == 0:\n",
    "            #print(loss.item())\n",
    "        optim_model.step()\n",
    "        #sched.step()\n",
    "        del loss\n",
    "        #result = result + torch.sum(torch.sum(torch.mul(torch.abs(torch.subtract(pred, label)), target_mask), dim = 0)).cpu()\n",
    "        #sum_labels = sum_labels + torch.sum(target_mask, dim=0).cpu()\n",
    "    cost = time.time() - start\n",
    "    return tr_loss, cost\n",
    "##%%\n",
    "\n",
    "def train(model, optim_model, trainload, valload, device):\n",
    "    with open(\"v_model_log_train.txt\", 'w') as f:\n",
    "            f.write('')\n",
    "    best_val = math.inf\n",
    "    for e in range(train_params[\"epochs\"]):\n",
    "        print(\"Epoch n\" + str(e))\n",
    "        train_loss, train_time_cost = run_epoch(e, model, optim_model, trainload, device)\n",
    "        val_loss, val_time_cost,pred, label = eval(model, optim_model, valload, False, device)\n",
    "        train_loss = (train_loss * train_params['batch_size']) / len(trainload)\n",
    "        val_loss = (val_loss * train_params['batch_size']) / len(valload)\n",
    "        print('TRAIN {}\\t{} secs\\n'.format(train_loss, train_time_cost))\n",
    "        with open(\"v_behrt_log_train.txt\", 'a') as f:\n",
    "            f.write(\"Epoch n\" + str(e) + '\\n TRAIN {}\\t{} secs\\n'.format(train_loss, train_time_cost))\n",
    "            f.write('EVAL {}\\t{} secs\\n'.format(val_loss, val_time_cost) + '\\n\\n\\n')\n",
    "        print('EVAL {}\\t{} secs\\n'.format(val_loss, val_time_cost))\n",
    "        if val_loss < best_val:\n",
    "            #print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            save_model(model_to_save.state_dict(), 'v_model')\n",
    "            best_val = val_loss\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "##%%\n",
    "\n",
    "def eval(model, optim_model, _valload, saving, device):\n",
    "    tr_loss = 0\n",
    "    tr_g_loss = 0\n",
    "    tr_d_un = 0\n",
    "    tr_d_sup = 0\n",
    "    temp_loss = 0\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    if saving:\n",
    "        with open(\"/content/drive/My Drive/GANmodel/preds/v_model_preds.csv\", 'w') as f:\n",
    "            f.write('')\n",
    "        with open(\"/content/drive/My Drive/GANmodel/preds/v_behrt_labels.csv\", 'w') as f:\n",
    "            f.write('')\n",
    "        '''\n",
    "        with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_masks.csv\", 'w') as f:\n",
    "            f.write('')\n",
    "        '''\n",
    "    for step, data in enumerate(_valload):\n",
    "        optim_model.zero_grad()\n",
    "\n",
    "        batched_data = Batch()\n",
    "        graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
    "        graph_batch = graph_batch.to(device)\n",
    "        nodes = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        edge_index_readout = graph_batch.edge_index\n",
    "        edge_attr = graph_batch.edge_attr\n",
    "        batch = graph_batch.batch\n",
    "        age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
    "        time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
    "        type_ids = torch.reshape(graph_batch.type, [graph_batch.type.shape[0] // 50, 50])\n",
    "        posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
    "        attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
    "        attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
    "\n",
    "        labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, 0].float()\n",
    "        #masks = torch.reshape(graph_batch.mask, [graph_batch.mask.shape[0] // 50, 50])[:, 0]\n",
    "        loss, logits = model(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids,type_ids,posi_ids,attMask, labels)\n",
    "\n",
    "        if saving:\n",
    "            with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_preds.csv\", 'a') as f:\n",
    "                pd.DataFrame(logits.detach().cpu().numpy()).to_csv(f, header=False)\n",
    "            with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_labels.csv\", 'a') as f:\n",
    "                pd.DataFrame(labels.detach().cpu().numpy()).to_csv(f, header=False)\n",
    "            \"\"\"\n",
    "            with open(\"/content/drive/My Drive/GANBEHRT/preds/v_behrt_masks.csv\", 'a') as f:\n",
    "                pd.DataFrame(masks.detach().cpu().numpy()).to_csv(f, header=False)\n",
    "            \"\"\"\n",
    "        tr_loss += loss.item()\n",
    "        del loss\n",
    "\n",
    "    #print(\"TOTAL LOSS\",(tr_loss * train_params['batch_size']) / len(_valload))\n",
    "\n",
    "    cost = time.time() - start\n",
    "    return tr_loss, cost, logits, labels #, masks\n",
    "\n",
    "\n",
    "def test(testload, model, optim_model, device):\n",
    "    model.eval()\n",
    "    tr_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    all_masks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(testload):\n",
    "            # Process the batch data and move it to the device\n",
    "            batched_data = Batch()\n",
    "            graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
    "            graph_batch = graph_batch.to(device)\n",
    "\n",
    "            # Extract relevant data from the batch\n",
    "            nodes = graph_batch.x\n",
    "            edge_index = graph_batch.edge_index\n",
    "            edge_index_readout = graph_batch.edge_index\n",
    "            edge_attr = graph_batch.edge_attr\n",
    "            batch = graph_batch.batch\n",
    "            age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
    "            time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
    "            type_ids = torch.reshape(graph_batch.type, [graph_batch.type.shape[0] // 50, 50])\n",
    "            posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
    "            attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
    "            attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
    "\n",
    "            labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, 0].float()\n",
    "            #masks = torch.reshape(graph_batch.mask, [graph_batch.mask.shape[0] // 50, 50])[:, 0]\n",
    "\n",
    "            # Forward pass\n",
    "            loss, logits = model(nodes, edge_index, edge_index_readout, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, attMask, labels)\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            del loss\n",
    "            # Accumulate logits, labels, and masks for later evaluation\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(\"TOTAL LOSS (test)\", (tr_loss * train_params['batch_size']) / len(testload))\n",
    "    cost = time.time() - start\n",
    "\n",
    "    # Calculate AUROC\n",
    "    auroc = roc_auc_score(all_labels, all_logits)\n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, all_logits)    \n",
    "    auprc = auc(recall, precision)\n",
    "    threshold = 0.5\n",
    "    #predicted_probabilities = [torch.sigmoid(torch.tensor(l)) for l in all_logits]\n",
    "    predicted_labels = [1 if l > threshold else 0 for l in all_logits]\n",
    "    f1 = f1_score(all_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    return auroc, auprc, f1\n",
    "\n",
    "def experiment(num_experiments=5):\n",
    "    conf = BertConfig(model_config)\n",
    "    model = BertForMTR(conf)\n",
    "    model = model.to(train_params['device'])\n",
    "    transformer_vars = [i for i in model.parameters()]\n",
    "    optim_model = torch.optim.AdamW(transformer_vars, lr=3e-5)\n",
    "    df = pd.DataFrame(columns=['Experiment', 'Model', 'Metric', 'Score'])\n",
    "    for exp in tqdm(range(num_experiments)):\n",
    "        print(f\"Experiment {exp + 1}\")\n",
    "        trainDSet, valDSet, testDSet = split_dataset(dataset, random_seed=exp)\n",
    "        trainload =  GraphLoader(GDSet(trainDSet), batch_size=train_params['batch_size'], shuffle=False)\n",
    "        valload =  GraphLoader(GDSet(valDSet), batch_size=train_params['batch_size'], shuffle=False)\n",
    "        testload =  GraphLoader(GDSet(testDSet), batch_size=train_params['batch_size'], shuffle=False)\n",
    "        train_loss, val_loss = train(model, optim_model, trainload, valload, train_params['device'])\n",
    "        auc_roc, aupcr, f1 = test(testload, model, optim_model, train_params['device'])\n",
    "        df.loc[len(df)] = [exp + 1, 'BERT', 'AUROC', auc_roc]\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_model(_model_dict, file_name):\n",
    "    torch.save(_model_dict, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:10:17.166967Z",
     "start_time": "2024-01-11T16:10:17.159722Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:10:17.571031Z",
     "start_time": "2024-01-11T16:10:17.568036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:10:34.354541Z",
     "start_time": "2024-01-11T16:10:17.957198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1\n",
      "Epoch n0\n",
      "TRAIN 2.401689833578835\t262.1649055480957 secs\n",
      "\n",
      "EVAL 2.2661300136791014\t13.699236154556274 secs\n",
      "\n",
      "Epoch n1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time as time \n",
    "df = experiment(num_experiments=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:11:06.777571Z",
     "start_time": "2024-01-11T16:11:06.770550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Experiment Model Metric  Score\n",
      "0           1  BERT  AUROC   0.25\n",
      "1           2  BERT  AUROC   1.00\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:11:25.648376Z",
     "start_time": "2024-01-11T16:11:25.641958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model Metric  Average Score  Standard Deviation\n",
      "0  BERT  AUROC           0.62                0.53\n"
     ]
    }
   ],
   "source": [
    "# Group by Model and Metric and calculate average and standard deviation\n",
    "result_df = df.groupby(['Model', 'Metric']).agg({'Score': ['mean', 'std']}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "result_df.columns = ['Model', 'Metric', 'Average Score', 'Standard Deviation']\n",
    "\n",
    "result_df['Average Score'] = result_df['Average Score'].round(2)\n",
    "result_df['Standard Deviation'] = result_df['Standard Deviation'].round(2)\n",
    "\n",
    "# Print the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T16:11:30.404747Z",
     "start_time": "2024-01-11T16:11:30.389770Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_parameters\u001b[39m(model):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m----> 3\u001b[0m count_parameters(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters())\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
