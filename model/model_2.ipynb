{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:48:57.119036Z",
     "start_time": "2024-01-21T16:48:57.113913Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import numpy as np\n",
    "import sparse\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as tgmnn\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.loader import DataListLoader as GraphLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "import time as time\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import sklearn.metrics as skm\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pytorch_pretrained_bert as Bert\n",
    "import itertools\n",
    "from einops import rearrange, repeat\n",
    "import ast\n",
    "from typing import Optional, Tuple, Union\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, SparseTensor\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn import LayerNorm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import transformers\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:48:57.666043Z",
     "start_time": "2024-01-21T16:48:57.658201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you are using a mac this cell aims to solve this error: \n",
    "# NotImplementedError: The operator 'aten::scatter_reduce.two_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable ⁠ PYTORCH_ENABLE_MPS_FALLBACK=1 ⁠ to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
    "\n",
    "# You will probably need first to execute this bash command in your terminal: touch .zshenv && echo 'export PYTORCH_ENABLE_MPS_FALLBACK=1' >> .zshenv\n",
    "# And then restart the notebook\n",
    "\n",
    "#import os\n",
    "#os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T19:09:19.256202Z",
     "start_time": "2024-01-21T19:09:19.250950Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerConv(MessagePassing):\n",
    "    _alpha: OptTensor\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        heads: int = 1,\n",
    "        concat: bool = True,\n",
    "        beta: bool = False,\n",
    "        dropout: float = 0.,\n",
    "        edge_dim: Optional[int] = None,\n",
    "        bias: bool = True,\n",
    "        root_weight: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.beta = beta and root_weight\n",
    "        self.root_weight = root_weight\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.edge_dim = edge_dim\n",
    "        self._alpha = None\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_key = Linear(in_channels[0], heads * out_channels)\n",
    "        self.lin_query = Linear(in_channels[1], heads * out_channels)\n",
    "        self.lin_value = Linear(in_channels[0], heads * out_channels)\n",
    "        self.layernorm1 = LayerNorm(out_channels)\n",
    "        self.layernorm2 = LayerNorm(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = Linear(heads * out_channels, out_channels)\n",
    "        self.ffn = Linear(out_channels, out_channels)\n",
    "        self.ffn2 = Linear(out_channels, out_channels)\n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
    "        else:\n",
    "            self.lin_edge = self.register_parameter('lin_edge', None)\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        self.lin_key.reset_parameters()\n",
    "        self.lin_query.reset_parameters()\n",
    "        self.lin_value.reset_parameters()\n",
    "        if self.edge_dim:\n",
    "            self.lin_edge.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, batch=None, return_attention_weights=None):\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, OptTensor, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, OptTensor, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        r\"\"\"Runs the forward pass of the module.\n",
    "\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "        residual = x\n",
    "        x = self.layernorm1(x, batch)\n",
    "        if isinstance(x, Tensor):\n",
    "            x: PairTensor = (x, x)\n",
    "        query = self.lin_query(x[1]).view(-1, H, C)\n",
    "        key = self.lin_key(x[0]).view(-1, H, C)\n",
    "        value = self.lin_value(x[0]).view(-1, H, C)\n",
    "        # propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa\n",
    "        out = self.propagate(edge_index, query=query, key=key, value=value,\n",
    "                             edge_attr=edge_attr, size=None)\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "        if self.concat:\n",
    "            out = self.proj(out.view(-1, self.heads * self.out_channels))\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out = out+residual\n",
    "        residual = out\n",
    "\n",
    "        out = self.layernorm2(out)\n",
    "        out = self.gelu(self.ffn(out))\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out = self.ffn2(out)\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        out = out + residual\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n",
    "                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "\n",
    "\n",
    "        if self.lin_edge is not None:\n",
    "            assert edge_attr is not None\n",
    "            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads,\n",
    "                                                      self.out_channels)\n",
    "            key_j = key_j + edge_attr\n",
    "\n",
    "        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = value_j\n",
    "        if edge_attr is not None:\n",
    "            out = out + edge_attr\n",
    "\n",
    "        out = out * alpha.view(-1, self.heads, 1)\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformerconv1 = TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=True)\n",
    "        self.transformerconv2 = TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=True)\n",
    "        self.transformerconv3 = TransformerConv(config.hidden_size // 5, config.hidden_size // 5, heads=2, edge_dim=config.hidden_size // 5, dropout=config.hidden_dropout_prob, concat=False)\n",
    "\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size // 5)\n",
    "        self.embed_ee = nn.Embedding(config.node_attr_size, config.hidden_size // 5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        indices = (x==0).nonzero().squeeze()\n",
    "        h_nodes = self.transformerconv1(x=self.embed(x), edge_index=edge_index, edge_attr=self.embed_ee(edge_attr), batch=batch)\n",
    "        h_nodes = nn.GELU()(h_nodes)\n",
    "        h_nodes = self.transformerconv2(x=h_nodes, edge_index=edge_index, edge_attr=self.embed_ee(edge_attr), batch=batch)\n",
    "        h_nodes = nn.GELU()(h_nodes)\n",
    "        h_nodes = self.transformerconv3(x=h_nodes, edge_index=edge_index, edge_attr=self.embed_ee(edge_attr), batch=batch)\n",
    "        x = h_nodes[indices]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, segment, age\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        #self.word_embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "        self.word_embeddings = GraphTransformer(config)\n",
    "        self.type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size//5)\n",
    "        self.age_embeddings = nn.Embedding(config.age_vocab_size, config.hidden_size//5). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.age_vocab_size, config.hidden_size//5))\n",
    "        self.time_embeddings = nn.Embedding(config.time_vocab_size , config.hidden_size//5). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.time_vocab_size, config.hidden_size//5))\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size//5). \\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size//5))\n",
    "\n",
    "\n",
    "        self.seq_layers = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_size),\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.acti = nn.GELU()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "\n",
    "    def forward(self, nodes, edge_index,  edge_attr, batch, age_ids, time_ids,  type_ids, posi_ids):\n",
    "        word_embed = self.word_embeddings(nodes, edge_index, edge_attr, batch)\n",
    "        type_embeddings = self.type_embeddings(type_ids)\n",
    "        age_embed = self.age_embeddings(age_ids)\n",
    "        time_embeddings = self.time_embeddings(time_ids)\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "\n",
    "        word_embed = torch.reshape(word_embed, type_embeddings.shape)\n",
    "        embeddings = torch.cat((word_embed, type_embeddings, posi_embeddings, age_embed, time_embeddings), dim=2)\n",
    "        \n",
    "        b, n, _ = embeddings.shape\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "        embeddings = self.seq_layers(embeddings)\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos / (10000 ** (2 * idx / hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos / (10000 ** (2 * idx / hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "\n",
    "\n",
    "\n",
    "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config=config)\n",
    "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
    "        self.pooler = Bert.modeling.BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, nodes, edge_index, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, attention_mask=None, output_all_encoded_layers=True):\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(age_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(nodes, edge_index, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output, extended_attention_mask, output_all_encoded_layers=output_all_encoded_layers)\n",
    "        \n",
    "        sequence_output = encoded_layers[-1]\n",
    "\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        \n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BertForMTR(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMTR, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "    def forward(self, nodes, edge_index, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, device, attention_mask=None, labels=None):\n",
    "        encoded_layer, pooled_output = self.bert(nodes, edge_index, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        logits = self.classifier(pooled_output).squeeze(dim=1)\n",
    "\n",
    "        \n",
    "        #weights = torch.where(labels == 1, torch.tensor(2, dtype=torch.long).to(device), torch.tensor(1, dtype=torch.long).to(device))  # more important weight in the loss for the patients that die\n",
    "        bce_logits_loss = nn.BCEWithLogitsLoss(reduction='mean') #, weight=weights\n",
    "        discr_supervised_loss = bce_logits_loss(logits, labels)\n",
    "        \n",
    "        return encoded_layer, pooled_output, logits, discr_supervised_loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.type_vocab_size = config.get('type_vocab_size')\n",
    "        self.time_vocab_size = config.get('time_vocab_size')\n",
    "        self.graph_dropout_prob = config.get('graph_dropout_prob')\n",
    "        self.node_attr_size = config.get('node_attr_size')\n",
    "\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')\n",
    "\n",
    "\n",
    "\n",
    "class GDSet(Dataset):\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        g = self.g[index]\n",
    "        for i in range(len(g)):\n",
    "          g[i]['posi_ids'] = i\n",
    "        return g\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:09:23.256100Z",
     "start_time": "2024-01-21T16:09:23.249845Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '../../data/'\n",
    "path_results = '../../results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:19:17.994521Z",
     "start_time": "2024-01-21T16:09:23.545443Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(path + 'data_pad_after_min10.pkl', 'rb') as handle:\n",
    "    dataset_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:19:17.999361Z",
     "start_time": "2024-01-21T16:19:17.994521Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset=dataset_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256\n",
      "2260\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "dic_noeud = {}\n",
    "dic_noeud[0] = 0\n",
    "\n",
    "for patient in dataset:\n",
    "    for visite in patient:\n",
    "        liste_noeud = []\n",
    "        for noeud in visite.x.tolist():\n",
    "            if noeud == 0:\n",
    "                liste_noeud.append(0)\n",
    "            else:\n",
    "                if noeud not in dic_noeud.keys():\n",
    "                    dic_noeud[noeud] = n\n",
    "                    n += 1\n",
    "                liste_noeud.append(dic_noeud[noeud])\n",
    "        visite.x = torch.tensor(liste_noeud, dtype=torch.int64)\n",
    "\n",
    "print(len(dic_noeud))\n",
    "print(max(dic_noeud.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:19:18.001510Z",
     "start_time": "2024-01-21T16:19:17.996603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367\n",
      "50\n",
      "Data(subject_id=[1], hadm_id=[1], label=[1], age=[1], rang=[1], type=[1], x=[47], edge_index=[2, 1081], edge_attr=[1081], mask_v=[1], time=[1])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(dataset[0]))\n",
    "print(dataset[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:19:21.322166Z",
     "start_time": "2024-01-21T16:19:18.072233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio label 0.2506811989100817\n",
      "vocab_size 2256\n",
      "max noeud 2260\n",
      "node_attr_size 8\n",
      "max edge_attr 7\n",
      "age_size 67\n",
      "max age 130\n",
      "time_size 367\n",
      "max time 367\n",
      "type_size 11\n",
      "max type 10\n",
      "label_size 2\n",
      "max label 1\n",
      "hadm_size 5300\n",
      "subject_size 367\n",
      "maskv_size 2\n",
      "max maskv 1\n",
      "rang_size 51\n",
      "max rang 51\n"
     ]
    }
   ],
   "source": [
    "noeud_unique = set()\n",
    "edge_attr_unique = set()\n",
    "age_unique = set()\n",
    "time_unique = set()\n",
    "type_unique = set()\n",
    "label_unique = set()\n",
    "hadm_unique = set()\n",
    "subject_unique = set()\n",
    "mask_v_unique = set()\n",
    "rang_unique = set()\n",
    "ratio_label = 0\n",
    "for patient in dataset:\n",
    "    ratio_label += patient[-1].label[0].item()\n",
    "    for visite in patient:\n",
    "        \n",
    "        noeuds = visite.x.tolist()\n",
    "        edge = visite.edge_attr.tolist()\n",
    "        label = visite.label.tolist()\n",
    "        age = visite.age.tolist()\n",
    "        time = visite.time.tolist()\n",
    "        typ = visite.type.tolist()\n",
    "        mask_v = visite.mask_v.tolist()\n",
    "        rang = visite.rang.tolist()\n",
    "        hadm = visite.hadm_id.tolist()\n",
    "        subject = visite.subject_id.tolist()\n",
    "        for noeud in noeuds:\n",
    "            noeud_unique.add(noeud)\n",
    "        for attribut in edge:\n",
    "            edge_attr_unique.add(attribut)\n",
    "        for lab in label:\n",
    "            label_unique.add(lab)\n",
    "            \n",
    "        for a in age:\n",
    "            age_unique.add(a)\n",
    "        for t in time:\n",
    "            time_unique.add(t)\n",
    "        for ty in typ:\n",
    "            type_unique.add(ty)\n",
    "        for mask in mask_v:\n",
    "            mask_v_unique.add(mask)\n",
    "        for r in rang:\n",
    "            rang_unique.add(r)\n",
    "        for h in hadm:\n",
    "            hadm_unique.add(h)\n",
    "        for s in subject:\n",
    "            subject_unique.add(s)\n",
    "ratio_label = ratio_label/len(dataset)\n",
    "print('ratio label',ratio_label)\n",
    "        \n",
    "\n",
    "vocab_size = len(noeud_unique)\n",
    "edge_attr_size = len(edge_attr_unique)\n",
    "age_size = len(age_unique)\n",
    "time_size = len(time_unique)\n",
    "type_size = len(type_unique)\n",
    "label_size = len(label_unique)\n",
    "hadm_size = len(hadm_unique)\n",
    "subject_size = len(subject_unique)\n",
    "mask_v_size = len(mask_v_unique)\n",
    "rang_size = len(rang_unique)\n",
    "\n",
    "print('vocab_size',vocab_size)\n",
    "print('max noeud',max(noeud_unique))\n",
    "print('node_attr_size',edge_attr_size)\n",
    "print('max edge_attr',max(edge_attr_unique))\n",
    "print('age_size',age_size)\n",
    "print('max age',max(age_unique))\n",
    "print('time_size',time_size)\n",
    "print('max time',max(time_unique))\n",
    "print('type_size',type_size)\n",
    "print('max type',max(type_unique))\n",
    "print('label_size',label_size)\n",
    "print('max label',max(label_unique))\n",
    "print('hadm_size',hadm_size)\n",
    "print('subject_size',subject_size)\n",
    "print('maskv_size',mask_v_size)\n",
    "print('max maskv',max(mask_v_unique))\n",
    "print('rang_size',rang_size)\n",
    "print('max rang',max(rang_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:19:21.328352Z",
     "start_time": "2024-01-21T16:19:21.322990Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_params, random_seed=1):\n",
    "  rs = ShuffleSplit(n_splits=1, test_size=train_params.get('test_split'), random_state=random_seed)\n",
    "\n",
    "  for i, (train_index_tmp, test_index) in enumerate(rs.split(dataset)):\n",
    "    rs2 = ShuffleSplit(n_splits=1, test_size=train_params.get('val_split'), random_state=random_seed)\n",
    "    \n",
    "    for j, (train_index, val_index) in enumerate(rs2.split(train_index_tmp)):\n",
    "      train_index = train_index_tmp[train_index]\n",
    "      val_index = train_index_tmp[val_index]\n",
    "      \n",
    "      trainDSet = [dataset[x] for x in train_index]\n",
    "      valDSet = [dataset[x] for x in val_index]\n",
    "      testDSet = [dataset[x] for x in test_index]\n",
    "\n",
    "      return trainDSet, valDSet, testDSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:19:21.349825Z",
     "start_time": "2024-01-21T16:19:21.325848Z"
    }
   },
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 10,\n",
    "    'max_len_seq': 50,\n",
    "    'device': \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"),\n",
    "    'data_len' : len(dataset),\n",
    "    'val_split' : 0.1,\n",
    "    'test_split' : 0.1,\n",
    "    'train_data_len' : int(0.9*0.9*len(dataset)),   # the train dataset is 90% of 80% of the whole dataset\n",
    "    'val_data_len' : int(0.1*0.9*len(dataset)),   # the validation dataset is 10% of 80% of the whole dataset\n",
    "    'test_data_len' : int(0.1*len(dataset)),   # the test dataset is 20% of the whole dataset\n",
    "    'epochs' : 150,\n",
    "    'lr': 1e-5,\n",
    "    'weight_decay': 0.0001,\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': 3769, # number of disease + symbols for word embedding\n",
    "    'edge_relationship_size': 8, # number of vocab for edge_attr\n",
    "    'hidden_size': 50*5, # word embedding and seg embedding hidden size\n",
    "    'age_vocab_size': 151, # number of vocab for age embedding\n",
    "    'time_vocab_size': 380, # number of vocab for time embedding\n",
    "    'type_vocab_size': 11+1, # number of vocab for type embedding + 1 for mask\n",
    "    'node_attr_size': 8, # number of vocab for node_attr embedding\n",
    "    'num_labels': 1,\n",
    "    'max_position_embedding': 50, # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'graph_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 2, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.2, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "    'n_layers' : 3 - 1,\n",
    "    'alpha' : 0.1,\n",
    "    'device': \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Upsampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T16:51:56.710265Z",
     "start_time": "2024-01-21T16:51:56.691460Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def upsampling(trainDSet):\n",
    "    start = time.time()\n",
    "    \n",
    "    trainSet2 = copy.deepcopy(trainDSet)\n",
    "    for patient in trainDSet:\n",
    "        if patient[0].label == 1:\n",
    "            trainSet2.append(patient)\n",
    "            \n",
    "    print(\"UPSAMPLING TIME\", time.time() - start)\n",
    "    return random.sample(trainSet2, len(trainSet2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:38:21.012512Z",
     "start_time": "2024-01-21T18:38:21.010591Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optim_model, trainload, device, writer, epoch):\n",
    "    tr_loss = 0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for step, data in tqdm(enumerate(trainload)):\n",
    "        optim_model.zero_grad()\n",
    "\n",
    "        batched_data = Batch()\n",
    "        graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
    "        graph_batch = graph_batch.to(device)\n",
    "        nodes = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        edge_attr = graph_batch.edge_attr\n",
    "        batch = graph_batch.batch\n",
    "        \n",
    "        age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
    "        time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
    "        type_ids = torch.reshape(graph_batch.type, [graph_batch.type.shape[0] // 50, 50])\n",
    "        posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
    "        attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
    "        attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
    "        labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, -1].float()\n",
    "\n",
    "        _, _, logits, loss = model(nodes, edge_index, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, device, attMask, labels)\n",
    "\n",
    "        # Update TensorBoard for train loss per iteration\n",
    "        writer.add_scalar('Train Loss per Iteration', loss.item(), epoch * len(trainload) + step)\n",
    "\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        optim_model.step()\n",
    "        del loss\n",
    "    \n",
    "    print(\"TOTAL TRAIN LOSS\",(tr_loss * train_params['batch_size']) / len(trainload))\n",
    "    cost = time.time() - start\n",
    "    print(\"TRAINING TIME\", cost)\n",
    "    \n",
    "    # Update TensorBoard for total train loss per epoch\n",
    "    writer.add_scalar('Total Train Loss per Epoch', (tr_loss * train_params['batch_size']) / len(trainload), epoch)\n",
    "\n",
    "    return tr_loss, cost\n",
    "\n",
    "\n",
    "def eval(model, optim_model, _valload, device, writer, epoch):\n",
    "    tr_loss = 0\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    all_pred = []\n",
    "    sig = nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(_valload):\n",
    "            optim_model.zero_grad()\n",
    "\n",
    "            batched_data = Batch()\n",
    "            graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
    "            graph_batch = graph_batch.to(device)\n",
    "            nodes = graph_batch.x\n",
    "            edge_index = graph_batch.edge_index\n",
    "            edge_attr = graph_batch.edge_attr\n",
    "            batch = graph_batch.batch\n",
    "\n",
    "            age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
    "            time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
    "            type_ids = torch.reshape(graph_batch.type, [graph_batch.type.shape[0] // 50, 50])\n",
    "            posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
    "            attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
    "            attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
    "            labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, 0].float()\n",
    "\n",
    "            _, _, logits, loss = model(nodes, edge_index, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, device, attMask, labels)\n",
    "            \n",
    "            # Update TensorBoard for eval loss per iteration\n",
    "            writer.add_scalar('Eval Loss per Iteration', loss.item(), epoch * len(_valload) + step)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            del loss\n",
    "\n",
    "            pred = np.where(sig(logits).cpu().detach().numpy() > 0.5, 1, 0)\n",
    "            all_pred.extend(pred)\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"TOTAL VAL LOSS\",(tr_loss * train_params['batch_size']) / len(_valload))\n",
    "    accuracy = skm.accuracy_score(all_labels, all_pred)\n",
    "    \n",
    "    # Update TensorBoard for total eval loss per epoch\n",
    "    writer.add_scalar('Total Eval Loss per Epoch', (tr_loss * train_params['batch_size']) / len(_valload), epoch)\n",
    "\n",
    "    cost = time.time() - start\n",
    "    print(\"EVAL TIME\", cost)\n",
    "\n",
    "    return tr_loss, cost, accuracy\n",
    "\n",
    "\n",
    "def test(testload, model, device, writer):\n",
    "    model.eval()\n",
    "    tr_loss = 0\n",
    "    start = time.time()\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    all_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(testload):\n",
    "            # Process the batch data and move it to the device\n",
    "            batched_data = Batch()\n",
    "            graph_batch = batched_data.from_data_list(list(itertools.chain.from_iterable(data)))\n",
    "            graph_batch = graph_batch.to(device)\n",
    "\n",
    "            # Extract relevant data from the batch\n",
    "            nodes = graph_batch.x\n",
    "            edge_index = graph_batch.edge_index\n",
    "            edge_attr = graph_batch.edge_attr\n",
    "            batch = graph_batch.batch\n",
    "\n",
    "            # extract age, time, type, posi, mask, label\n",
    "            age_ids = torch.reshape(graph_batch.age, [graph_batch.age.shape[0] // 50, 50])\n",
    "            time_ids = torch.reshape(graph_batch.time, [graph_batch.time.shape[0] // 50, 50])\n",
    "            type_ids = torch.reshape(graph_batch.type, [graph_batch.type.shape[0] // 50, 50])\n",
    "            posi_ids = torch.reshape(graph_batch.posi_ids, [graph_batch.posi_ids.shape[0] // 50, 50])\n",
    "            attMask = torch.reshape(graph_batch.mask_v, [graph_batch.mask_v.shape[0] // 50, 50])\n",
    "            attMask = torch.cat((torch.ones((attMask.shape[0], 1)).to(device), attMask), dim=1)\n",
    "            labels = torch.reshape(graph_batch.label, [graph_batch.label.shape[0] // 50, 50])[:, 0].float()\n",
    "\n",
    "            # Forward pass\n",
    "            _, _, logits, loss = model(nodes, edge_index, edge_attr, batch, age_ids, time_ids, type_ids, posi_ids, device, attMask, labels)\n",
    "            \n",
    "            # Update TensorBoard for eval loss per iteration\n",
    "            writer.add_scalar('Test Loss per Iteration', loss.item(), step)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            del loss\n",
    "\n",
    "            pred = torch.sigmoid(logits)\n",
    "            all_pred.extend(pred.detach().cpu().numpy())\n",
    "            # Accumulate logits, labels for later evaluation\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"TOTAL TEST LOSS \", (tr_loss * train_params['batch_size']) / len(testload))\n",
    "    cost = time.time() - start\n",
    "    print(\"TEST TEST TIME\", cost)\n",
    "    \n",
    "    # Update TensorBoard for total train loss per epoch\n",
    "    writer.add_scalar('Total Test Loss per Epoch', (tr_loss * train_params['batch_size']) / len(testload), 0)\n",
    "    \n",
    "    predicted_probabilities = [torch.sigmoid(torch.tensor(l)) for l in all_logits]\n",
    "    auroc = roc_auc_score(all_labels, predicted_probabilities)\n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, predicted_probabilities)   \n",
    "    apscore = average_precision_score(all_labels, predicted_probabilities)\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    predicted_labels = [1 if l > threshold else 0 for l in all_logits]\n",
    "    f1 = f1_score(all_labels, predicted_labels, average = 'weighted')\n",
    "    accuracy = skm.accuracy_score(all_labels, predicted_labels)\n",
    "\n",
    "    return auroc, auprc, f1, accuracy, cost, apscore\n",
    "\n",
    "\n",
    "\n",
    "def run_epoch(model, optim_model, trainload, valload, device, exp, writer):\n",
    "    best_val = math.inf\n",
    "    loss_train_liste = []\n",
    "    loss_val_liste = []\n",
    "\n",
    "    for e in tqdm(range(train_params[\"epochs\"])):\n",
    "        print(\"\\nEpoch n\" + str(e))\n",
    "\n",
    "        train_loss, train_time_cost = train(model, optim_model, trainload, device, writer, e)\n",
    "        val_loss, val_time_cost, accuracy = eval(model, optim_model, valload, device, writer, e)\n",
    "\n",
    "        train_loss = (train_loss * train_params['batch_size']) / len(trainload)\n",
    "        val_loss = (val_loss * train_params['batch_size']) / len(valload)\n",
    "        loss_train_liste.append(train_loss)\n",
    "        loss_val_liste.append(val_loss)\n",
    "        \n",
    "        with open(path_results + 'losses_and_times/' + \"GT_behrt_log_train_\" + f'{exp}' + \".txt\", 'a') as f:\n",
    "            f.write(\"Epoch n\" + str(e) + '\\n TRAIN {}\\t{} secs\\n'.format(train_loss, train_time_cost))\n",
    "            f.write('EVAL {}\\t{} secs\\n'.format(val_loss, val_time_cost) + '\\n\\n\\n')\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save(model_to_save.state_dict(), path_results + 'weights/' + 'GT_bert_num_' + f'{exp}' + '.pch')\n",
    "            # sauvegarder les parametre du model\n",
    "            best_val = val_loss\n",
    "\n",
    "\n",
    "    epoch = [i for i in range(train_params[\"epochs\"])]\n",
    "    plt.plot(epoch, loss_train_liste)\n",
    "    plt.legend(['train'])\n",
    "    plt.plot(epoch,loss_val_liste)\n",
    "    plt.legend(['val'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(path_results + 'plots/' + 'GT_bert.png')\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss, val_loss, accuracy, train_time_cost, val_time_cost\n",
    "\n",
    "\n",
    "\n",
    "def experiment( num_experiments=3):\n",
    "    conf = BertConfig(model_config)\n",
    "    df = pd.DataFrame(columns=['Experiment', 'Model', 'Metric', 'Score'])\n",
    "    df2 = pd.DataFrame(columns=['Experiment', 'Model', 'Metric', 'Score'])\n",
    "\n",
    "    for exp in tqdm(range(num_experiments)):\n",
    "        model = BertForMTR(conf).to(train_params['device'])\n",
    "        \n",
    "        # taking back the weights of the pretraining 2\n",
    "        #model.load_state_dict(torch.load(path_results + 'weights/' + 'GraphTransformer_pretrain_2_num_' + f'{exp}' + '.pch', map_location=train_params.get('device')))\n",
    "        \n",
    "        transformer_vars = [i for i in model.parameters()]\n",
    "        optim_model = torch.optim.AdamW(transformer_vars, lr=train_params['lr']) #, weight_decay=train_params['weight_decay']\n",
    "        scheduler = None\n",
    "        \n",
    "        # Set log directory for TensorBoard\n",
    "        current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        writer = SummaryWriter(path_results + 'runs/' + 'model/' + current_time + str(exp + 1))\n",
    "    \n",
    "        print(f\"\\n Experiment {exp + 1}\")\n",
    "        trainDSet, valDSet, testDSet = split_dataset(dataset, train_params, random_seed=exp)   #trainDSet\n",
    "        \n",
    "        # upsampling\n",
    "        #trainDSet = upsampling(trainDSet)\n",
    "        \n",
    "        trainload =  GraphLoader(GDSet(trainDSet), batch_size=train_params['batch_size'], shuffle=False) \n",
    "        valload =  GraphLoader(GDSet(valDSet), batch_size=train_params['batch_size'], shuffle=False)\n",
    "        testload =  GraphLoader(GDSet(testDSet), batch_size=train_params['batch_size'], shuffle=False)\n",
    "\n",
    "        train_loss, val_loss, accuracy_val ,train_time_cost, val_time_cost = run_epoch(model, optim_model, trainload, valload, train_params['device'], exp, writer)\n",
    "        \n",
    "        auc_roc, aupcr, f1, accuracy_test, cost , apscore = test(testload, model, train_params['device'], writer)\n",
    "\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Test AUROC', auc_roc]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Test AUPRC', aupcr]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Test F1', f1]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Val Accuracy', accuracy_val]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Test Accuracy', accuracy_test]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Train Loss', train_loss]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Val Loss', val_loss]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Train Time', train_time_cost]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Val Time', val_time_cost]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Test Time', cost]\n",
    "        df2.loc[len(df2)] = [exp + 1, 'GT_BERT', 'Test AP', apscore]\n",
    "        \n",
    "        # we take back the best model in order to use it for the test\n",
    "        model = BertForMTR(conf).to(train_params['device'])\n",
    "        model.load_state_dict(torch.load(path_results + 'weights/' + 'GT_bert_num_' + f'{exp}' + '.pch'))\n",
    "        \n",
    "        # we test on the best model\n",
    "        auc_roc, aupcr, f1, accuracy_test, cost , apscore = test(testload, model, train_params['device'], writer)\n",
    "\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Test AUROC', auc_roc]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Test AUPRC', aupcr]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Test F1', f1]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Val Accuracy', accuracy_val]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Test Accuracy', accuracy_test]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Train Loss', train_loss]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Val Loss', val_loss]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Train Time', train_time_cost]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Val Time', val_time_cost]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Test Time', cost]\n",
    "        df.loc[len(df)] = [exp + 1, 'GT_BERT', 'Test AP', apscore]\n",
    "        \n",
    "        # Close the writer for the current experiment\n",
    "        writer.close()\n",
    "        \n",
    "    df.to_csv(path_results + 'dataframes/' + 'GT_behrt_results.csv')\n",
    "\n",
    "    return df,df2, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T01:39:30.173631Z",
     "start_time": "2024-01-21T19:09:26.096778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Experiment 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch n0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "52it [01:05,  1.26s/it]\n",
      "  0%|          | 0/150 [01:05<?, ?it/s]\n",
      "  0%|          | 0/1 [01:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df, df2,model \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_experiments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[103], line 237\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(num_experiments)\u001b[0m\n\u001b[1;32m    234\u001b[0m valload \u001b[38;5;241m=\u001b[39m  GraphLoader(GDSet(valDSet), batch_size\u001b[38;5;241m=\u001b[39mtrain_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    235\u001b[0m testload \u001b[38;5;241m=\u001b[39m  GraphLoader(GDSet(testDSet), batch_size\u001b[38;5;241m=\u001b[39mtrain_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 237\u001b[0m train_loss, val_loss, accuracy_val ,train_time_cost, val_time_cost \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m auc_roc, aupcr, f1, accuracy_test, cost , apscore \u001b[38;5;241m=\u001b[39m test(testload, model, train_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m], writer)\n\u001b[1;32m    241\u001b[0m df2\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(df2)] \u001b[38;5;241m=\u001b[39m [exp \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGT_BERT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest AUROC\u001b[39m\u001b[38;5;124m'\u001b[39m, auc_roc]\n",
      "Cell \u001b[0;32mIn[103], line 174\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, optim_model, trainload, valload, device, exp, writer)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch n\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m--> 174\u001b[0m     train_loss, train_time_cost \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     val_loss, val_time_cost, accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, optim_model, valload, device, writer, e)\n\u001b[1;32m    177\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m (train_loss \u001b[38;5;241m*\u001b[39m train_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainload)\n",
      "Cell \u001b[0;32mIn[103], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim_model, trainload, device, writer, epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Update TensorBoard for train loss per iteration\u001b[39;00m\n\u001b[1;32m     28\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss per Iteration\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), epoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainload) \u001b[38;5;241m+\u001b[39m step)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     32\u001b[0m optim_model\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df, df2,model = experiment(num_experiments=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T18:56:02.873484Z",
     "start_time": "2024-01-22T18:56:02.846646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Model</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AUROC</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AUPRC</td>\n",
       "      <td>0.304887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test F1</td>\n",
       "      <td>0.675240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Accuracy</td>\n",
       "      <td>0.878788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test Accuracy</td>\n",
       "      <td>0.729730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Train Loss</td>\n",
       "      <td>2.192117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Loss</td>\n",
       "      <td>3.482292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Train Time</td>\n",
       "      <td>11.248456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Time</td>\n",
       "      <td>0.396784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test Time</td>\n",
       "      <td>1.072309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AP</td>\n",
       "      <td>0.357562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Experiment    Model         Metric      Score\n",
       "0            1  GT_BERT     Test AUROC   0.583333\n",
       "1            1  GT_BERT     Test AUPRC   0.304887\n",
       "2            1  GT_BERT        Test F1   0.675240\n",
       "3            1  GT_BERT   Val Accuracy   0.878788\n",
       "4            1  GT_BERT  Test Accuracy   0.729730\n",
       "5            1  GT_BERT     Train Loss   2.192117\n",
       "6            1  GT_BERT       Val Loss   3.482292\n",
       "7            1  GT_BERT     Train Time  11.248456\n",
       "8            1  GT_BERT       Val Time   0.396784\n",
       "9            1  GT_BERT      Test Time   1.072309\n",
       "10           1  GT_BERT        Test AP   0.357562"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Model</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AUROC</td>\n",
       "      <td>0.420635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AUPRC</td>\n",
       "      <td>0.216916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test F1</td>\n",
       "      <td>0.675240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Accuracy</td>\n",
       "      <td>0.878788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test Accuracy</td>\n",
       "      <td>0.729730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Train Loss</td>\n",
       "      <td>2.192117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Loss</td>\n",
       "      <td>3.482292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Train Time</td>\n",
       "      <td>11.248456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Time</td>\n",
       "      <td>0.396784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test Time</td>\n",
       "      <td>0.599990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AP</td>\n",
       "      <td>0.264808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Experiment    Model         Metric      Score\n",
       "0            1  GT_BERT     Test AUROC   0.420635\n",
       "1            1  GT_BERT     Test AUPRC   0.216916\n",
       "2            1  GT_BERT        Test F1   0.675240\n",
       "3            1  GT_BERT   Val Accuracy   0.878788\n",
       "4            1  GT_BERT  Test Accuracy   0.729730\n",
       "5            1  GT_BERT     Train Loss   2.192117\n",
       "6            1  GT_BERT       Val Loss   3.482292\n",
       "7            1  GT_BERT     Train Time  11.248456\n",
       "8            1  GT_BERT       Val Time   0.396784\n",
       "9            1  GT_BERT      Test Time   0.599990\n",
       "10           1  GT_BERT        Test AP   0.264808"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T18:56:07.060607Z",
     "start_time": "2024-01-22T18:56:07.041636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Average Score</th>\n",
       "      <th>Standard Deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AP</td>\n",
       "      <td>0.36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AUPRC</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test AUROC</td>\n",
       "      <td>0.58</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test Accuracy</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test F1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Test Time</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Train Loss</td>\n",
       "      <td>2.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Train Time</td>\n",
       "      <td>11.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Accuracy</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Loss</td>\n",
       "      <td>3.48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GT_BERT</td>\n",
       "      <td>Val Time</td>\n",
       "      <td>0.40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model         Metric  Average Score  Standard Deviation\n",
       "0   GT_BERT        Test AP           0.36                 NaN\n",
       "1   GT_BERT     Test AUPRC           0.30                 NaN\n",
       "2   GT_BERT     Test AUROC           0.58                 NaN\n",
       "3   GT_BERT  Test Accuracy           0.73                 NaN\n",
       "4   GT_BERT        Test F1           0.68                 NaN\n",
       "5   GT_BERT      Test Time           1.07                 NaN\n",
       "6   GT_BERT     Train Loss           2.19                 NaN\n",
       "7   GT_BERT     Train Time          11.25                 NaN\n",
       "8   GT_BERT   Val Accuracy           0.88                 NaN\n",
       "9   GT_BERT       Val Loss           3.48                 NaN\n",
       "10  GT_BERT       Val Time           0.40                 NaN"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by Model and Metric and calculate average and standard deviation\n",
    "result_df = df.groupby(['Model', 'Metric']).agg({'Score': ['mean', 'std']}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "result_df.columns = ['Model', 'Metric', 'Average Score', 'Standard Deviation']\n",
    "\n",
    "result_df['Average Score'] = result_df['Average Score'].round(2)\n",
    "result_df['Standard Deviation'] = result_df['Standard Deviation'].round(2)\n",
    "\n",
    "# Print the result\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T18:56:08.399073Z",
     "start_time": "2024-01-22T18:56:08.391310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3552773"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T18:56:09.650359Z",
     "start_time": "2024-01-22T18:56:09.641145Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving results\n",
    "result_df.to_csv(path_results + 'dataframes/' + 'GT_behrt_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.to_csv(path_results + 'dataframes/' + 'GT_behrt_results2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
